{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import scipy\n",
    "import shutil\n",
    "from glob import glob\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from wfdb import processing\n",
    "from numpy import genfromtxt\n",
    "from scipy.signal import filtfilt\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPool1D\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Reshape\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2'"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Get Paths of the .atr files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records():\n",
    "    paths = glob('data1/*.atr') #list of all the data files. \n",
    "    paths = [path[:-4] for path in paths]\n",
    "    paths.sort()\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Read the .atr files and generate signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signals(records): #There's 48 total records. If not specified, we'll process every single record \n",
    "    signals = []\n",
    "    for e in records:\n",
    "        if '-' in e: continue      \n",
    "        sig, fields = wfdb.rdsamp(e, channels=[0]) \n",
    "        signals.append(np.array([x[0] for x in sig])) \n",
    "    return  signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Detect the QRS Complex in signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qrs_detect(records): #There's 48 total records. If not specified, we'll process every single record \n",
    "    qrs_inds = []\n",
    "    signals = []\n",
    "    for e in records:\n",
    "        if '-' in e: continue     \n",
    "        sig, fields = wfdb.rdsamp(e, channels=[0]) \n",
    "        signals.append(np.array([x[0] for x in sig])) \n",
    "        qrs_ind = processing.xqrs_detect(sig=sig[:,0], fs=fields['fs'])\n",
    "        qrs_inds.append(qrs_ind) \n",
    "    return qrs_inds, signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Segment the QRS Complex in signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_QRS(qrs_inds, signals):\n",
    "    prev_ind = 0    # Lower bound on segment \n",
    "    end_ind = 0;  # Upper bound on segment \n",
    "    last_ind = qrs_inds[-1] # Last index in qrs_inds. Used for edge case \n",
    "    segments = []   # List of numpy arrays representing ONE patient's QRS complexes\n",
    "    one_behind = 0\n",
    "    for ind in qrs_inds[0]: \n",
    "        if ind == qrs_inds[0][0]: continue  \n",
    "        if ind == qrs_inds[0][-1]: \n",
    "            segments.append(signals[prev_ind:len(signals) - 1])\n",
    "            continue\n",
    "        end_ind = ((qrs_inds[0][one_behind] + ind) // 2) - 1\n",
    "        segments.append(signals[prev_ind:end_ind])\n",
    "        prev_ind = end_ind + 1\n",
    "        one_behind = one_behind + 1\n",
    "    return segments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(qrs_segments, record_name):\n",
    "    filepath = os.getcwd() + '/encoded_record_segments/' \n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    sub_filepath = filepath + record_name \n",
    "    if not os.path.exists(sub_filepath): \n",
    "        os.makedirs(sub_filepath) \n",
    "    if os.path.exists(sub_filepath + '.zip'):\n",
    "        print('{0}\\'s directory already exists. Skipping this record.'.format(record_name))\n",
    "        return \n",
    "    seg_filepath = sub_filepath + '/seg'\n",
    "    counter = 0  \n",
    "    for qrs_complex in qrs_segments:\n",
    "        label_encoder = LabelEncoder() \n",
    "        integer_encoded = label_encoder.fit_transform(qrs_complex) \n",
    "        onehot_encoder = OneHotEncoder(sparse=False) \n",
    "        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1) \n",
    "        onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "        np.save(seg_filepath + str(counter), onehot_encoded)    # Saving as a binary file (numpy array) \n",
    "        counter = counter + 1 \n",
    "    shutil.make_archive(sub_filepath, 'zip', sub_filepath)\n",
    "    shutil.rmtree(sub_filepath) # Removes the original directory to save space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Extract Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_records(deleteZips=True):\n",
    "    all_zip_files = os.listdir(os.getcwd() + '/encoded_record_segments')\n",
    "    extract_dir = os.getcwd() + '/encoded_record_segments'\n",
    "    for file in all_zip_files:\n",
    "        filename = extract_dir + '/' + file\n",
    "        shutil.unpack_archive(filename, filename[:-4], 'zip')\n",
    "        if deleteZips is True: \n",
    "            os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "INPUT_SIZE = 1250 #Number of features in 1 timestep\n",
    "# NUM_CLASSES = 2 #Either is valid authentication or isn't. \n",
    "NUM_CLASSES = 5\n",
    "DROPOUT_VALUE = 0.5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(100, 10, activation='relu', input_shape=(1250, 1)))\n",
    "model.add(Conv1D(100, 10, activation='relu'))\n",
    "model.add(MaxPool1D(3)) \n",
    "model.add(Conv1D(160, 10, activation='relu'))\n",
    "model.add(Conv1D(160, 10, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(DROPOUT_VALUE))\n",
    "model.add(Dense(units=NUM_CLASSES, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Patition data into Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data(no_records_for_input):\n",
    "    test_data, test_labels = [], []\n",
    "    filepath = os.getcwd() + '/all_signals/'\n",
    "    data = genfromtxt(filepath + '100.csv', delimiter=',')\n",
    "    train_index = int(len(data) * 0.8)\n",
    "    data_index = len(data[0])\n",
    "    print(len(data))\n",
    "    train_data = [x[0:data_index] for x in data[0:train_index]]\n",
    "    test_data = [x[0:data_index] for x in data[train_index + 1:]]\n",
    "\n",
    "    train_labels = []\n",
    "    test_labels = []\n",
    "\n",
    "    record_names = [str(x) + '.csv' for x in range(101, 105) if x != 110] \n",
    "    for record in record_names:\n",
    "        data = genfromtxt(filepath + record, delimiter=',')\n",
    "        #print(len(data))\n",
    "        #negative will follow as: 160 records from each record for training and 40 for test.\n",
    "        neg_train_data = [x[0:data_index] for x in data[0:train_index]]\n",
    "        neg_test_data = [x[0:data_index] for x in data[train_index+1:]]\n",
    "        \n",
    "        for test_d in neg_test_data:\n",
    "            test_data.append(test_d)\n",
    "\n",
    "        for train_d in neg_train_data:\n",
    "            train_data.append(train_d)\n",
    "    \n",
    "    np.random.shuffle(train_data)\n",
    "    np.random.shuffle(test_data) \n",
    "    train_labels = np.array([int(x[-1]) for x in train_data])\n",
    "    test_labels = np.array([int(x[-1]) for x in test_data])\n",
    "    train_data = np.array([x[0:data_index - 1] for x in train_data])\n",
    "    test_data = np.array([x[0:data_index - 1] for x in test_data])\n",
    "    print(\"Unique values of test\", np.unique(test_labels, return_counts=True), len(test_labels))\n",
    "    \n",
    "    \n",
    "    print(\"Original Train Labels:\", train_labels, len(train_labels))\n",
    "#     train_labels = keras.utils.to_categorical(train_labels)\n",
    "    label_encoder = LabelEncoder()\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    train_encoded = label_encoder.fit_transform(train_labels)\n",
    "    print(\"Label encoded Train Labels:\", train_encoded, len(train_encoded))\n",
    "    train_encoded = train_encoded.reshape(len(train_encoded), 1)\n",
    "    train_labels = onehot_encoder.fit_transform(train_encoded)\n",
    "    print(\"Train Labels after keras:\", train_labels)\n",
    "    print(\"Length in one list of train_label:\", len(train_labels[0]))\n",
    "    \n",
    "    print(\"Original test Labels:\", test_labels)\n",
    "    test_encoded = label_encoder.fit_transform(test_labels)\n",
    "    test_encoded = test_encoded.reshape(len(test_encoded), 1)\n",
    "    test_labels = onehot_encoder.fit_transform(test_encoded)\n",
    "#   test_labels = keras.utils.to_categorical(test_labels)\n",
    "    print(\"Test Lables after encoding:\", test_labels)\n",
    "    \n",
    "    train_records=no_records_for_input*416\n",
    "    test_records=no_records_for_input*103\n",
    "    train_data = train_data.reshape((train_records, 1250, 1))\n",
    "    test_data = test_data.reshape((test_records, 1250, 1))\n",
    "    #Save files as binary files to save time on training. \n",
    "    np.save(filepath + 'traindata', train_data)\n",
    "    np.save(filepath + 'trainlabels', train_labels)\n",
    "    np.save(filepath + 'testdata', test_data)\n",
    "    np.save(filepath + 'testlabels', test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color =blue>Main Function</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total No of records to be tested for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_records_for_input=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data1/100',\n",
       " 'data1/101',\n",
       " 'data1/102',\n",
       " 'data1/103',\n",
       " 'data1/104',\n",
       " 'data1/105',\n",
       " 'data1/106',\n",
       " 'data1/107',\n",
       " 'data1/108',\n",
       " 'data1/109']"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records=get_records()\n",
    "records[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1D-CNN.ipynb',\n",
       " '.DS_Store',\n",
       " 'qrs_complex.py',\n",
       " 'preprocess.py',\n",
       " 'Dataset',\n",
       " 'data1 2',\n",
       " 'RAW_ECG_CSV',\n",
       " 'data1',\n",
       " 'BANDPASS_LP5_HP_35',\n",
       " '__pycache__',\n",
       " 'csv_segmented_individual',\n",
       " 'README.md',\n",
       " 'BANDPASS_LP5_HP_40',\n",
       " 'array.csv',\n",
       " 'all_signals',\n",
       " 'butterworth.py',\n",
       " 'Input.py',\n",
       " 'encoded_record_segments',\n",
       " '.ipynb_checkpoints',\n",
       " 'segmented_csvdata',\n",
       " 'data',\n",
       " 'BANDPASS_LP5_HP_30',\n",
       " 'b101.csv']"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cw=os.getcwd()\n",
    "cw\n",
    "os.listdir(cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = records[:no_records_for_input] #We only want the first 11 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.145, -0.145, -0.145, ..., -0.675, -0.765, -1.28 ]),\n",
       " array([-0.345, -0.345, -0.345, ..., -0.295, -0.29 ,  0.   ]),\n",
       " array([-0.2  , -0.2  , -0.2  , ..., -0.17 , -0.195,  0.   ]),\n",
       " array([-0.375, -0.375, -0.375, ..., -0.235, -0.245,  0.   ]),\n",
       " array([-0.15 , -0.15 , -0.15 , ..., -0.065, -0.06 ,  0.   ])]"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signals = get_signals(records)\n",
    "signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.getcwd() + '/all_signals/'\n",
    "if not os.path.exists(filepath):\n",
    "    os.makedirs(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning initial signal parameters...\n",
      "Found 8 beats during learning. Initializing using learned parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Learning initial signal parameters...\n",
      "Found 8 beats during learning. Initializing using learned parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Learning initial signal parameters...\n",
      "Found 8 beats during learning. Initializing using learned parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Learning initial signal parameters...\n",
      "Found 8 beats during learning. Initializing using learned parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Learning initial signal parameters...\n",
      "Found 8 beats during learning. Initializing using learned parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Learning initial signal parameters...\n",
      "Found 8 beats during learning. Initializing using learned parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Learning initial signal parameters...\n",
      "Found 8 beats during learning. Initializing using learned parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Learning initial signal parameters...\n",
      "Found 8 beats during learning. Initializing using learned parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Learning initial signal parameters...\n",
      "Found 8 beats during learning. Initializing using learned parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Learning initial signal parameters...\n",
      "Found 8 beats during learning. Initializing using learned parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Learning initial signal parameters...\n",
      "Found 8 beats during learning. Initializing using learned parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Learning initial signal parameters...\n",
      "Found 8 beats during learning. Initializing using learned parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Learning initial signal parameters...\n",
      "Found 8 beats during learning. Initializing using learned parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Learning initial signal parameters...\n",
      "Found 8 beats during learning. Initializing using learned parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Learning initial signal parameters...\n",
      "Found 8 beats during learning. Initializing using learned parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Learning initial signal parameters...\n",
      "Found 8 beats during learning. Initializing using learned parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "('QRS Complex indexes:', [array([    76,    370,    662, ..., 649484, 649733, 649992]), array([    83,    396,    711, ..., 649004, 649372, 649751]), array([   116,    410,    697, ..., 649243, 649553, 649851]), array([   265,    575,    876, ..., 649195, 649534, 649875]), array([   314,    613,    900, ..., 649299, 649579, 649875]), array([   196,    458,    708, ..., 649471, 649739, 649993]), array([   351,    724,   1086, ..., 649163, 649791, 649991]), array([   259,    567,    872, ..., 649116, 649428, 649735]), array([    89,    367,    442, ..., 649108, 649401, 649769]), array([   128,    359,    588, ..., 649362, 649647, 649916]), array([   198,    490,    804, ..., 649149, 649442, 649725]), array([   124,    382,    645, ..., 649626, 649883, 649994]), array([   171,    583,    966, ..., 649364, 649675, 649994]), array([   300,    739,   1156, ..., 649536, 649796, 649993]), array([   162,    518,    850, ..., 649358, 649647, 649955]), array([   282,    561,    837, ..., 649442, 649701, 649957])], 2273)\n"
     ]
    }
   ],
   "source": [
    " qrs_inds, signals = qrs_detect(records)\n",
    " print(\"QRS Complex indexes:\", qrs_inds, len(qrs_inds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record100's directory already exists. Skipping this record.\n",
      "record101's directory already exists. Skipping this record.\n",
      "record102's directory already exists. Skipping this record.\n",
      "record103's directory already exists. Skipping this record.\n",
      "record104's directory already exists. Skipping this record.\n",
      "record105's directory already exists. Skipping this record.\n",
      "record106's directory already exists. Skipping this record.\n",
      "record107's directory already exists. Skipping this record.\n",
      "record108's directory already exists. Skipping this record.\n",
      "record109's directory already exists. Skipping this record.\n",
      "record111's directory already exists. Skipping this record.\n",
      "record112's directory already exists. Skipping this record.\n",
      "record113's directory already exists. Skipping this record.\n",
      "record114's directory already exists. Skipping this record.\n",
      "record115's directory already exists. Skipping this record.\n",
      "record116's directory already exists. Skipping this record.\n"
     ]
    }
   ],
   "source": [
    "for qrs, sig, record in zip(qrs_inds, signals, records): \n",
    "        segments = segment_QRS(qrs_inds, sig) \n",
    "        one_hot_encode(segments, 'record' + record.split(\"/\",-1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2/all_signals/100.csv\n",
      "/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2/all_signals/101.csv\n",
      "/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2/all_signals/102.csv\n",
      "/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2/all_signals/103.csv\n",
      "/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2/all_signals/104.csv\n",
      "/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2/all_signals/105.csv\n",
      "/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2/all_signals/106.csv\n",
      "/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2/all_signals/107.csv\n",
      "/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2/all_signals/108.csv\n",
      "/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2/all_signals/109.csv\n",
      "/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2/all_signals/111.csv\n",
      "/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2/all_signals/112.csv\n",
      "/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2/all_signals/113.csv\n",
      "/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2/all_signals/114.csv\n",
      "/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2/all_signals/115.csv\n"
     ]
    }
   ],
   "source": [
    "INDEX_DIFF = 1249\n",
    "for record,signal in zip(records, signals):\n",
    "    number = 0\n",
    "    record = record.split(\"/\",-1)[1]\n",
    "    index = 0\n",
    "    l = []\n",
    "    sub_filepath = os.path.join(filepath, record+'.csv')\n",
    "    print(sub_filepath)\n",
    "    for x in range(520):\n",
    "            segmented = None\n",
    "            label=int(record)%100\n",
    "            segmented = np.append(signal[index:index + INDEX_DIFF + 1], [label])  \n",
    "            l.append(segmented)\n",
    "            index += INDEX_DIFF + 1\n",
    "            number += 1\n",
    "    np.savetxt(sub_filepath, l, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520\n",
      "('Unique values of test', (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 11, 12, 13, 14, 15]), array([103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103,\n",
      "       103, 103])), 1545)\n",
      "('Original Train Labels:', array([ 5, 15, 12, ..., 14, 13, 13]), 6240)\n",
      "('Label encoded Train Labels:', array([ 5, 14, 11, ..., 13, 12, 12]), 6240)\n",
      "('Train Labels after keras:', array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 1.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 1., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.]]))\n",
      "('Length in one list of train_label:', 15)\n",
      "('Original test Labels:', array([ 3, 13,  8, ..., 11,  4,  6]))\n",
      "('Test Lables after encoding:', array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]))\n"
     ]
    }
   ],
   "source": [
    "partition_data(no_records_for_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Records: Without Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4160/4160 [==============================] - 72s 17ms/step - loss: 1.8149 - accuracy: 0.2983\n",
      "Epoch 2/10\n",
      "4160/4160 [==============================] - 85s 21ms/step - loss: 0.8586 - accuracy: 0.6974\n",
      "Epoch 3/10\n",
      "4160/4160 [==============================] - 77s 18ms/step - loss: 0.3564 - accuracy: 0.8957\n",
      "Epoch 4/10\n",
      "4160/4160 [==============================] - 76s 18ms/step - loss: 0.2181 - accuracy: 0.9322\n",
      "Epoch 5/10\n",
      "4160/4160 [==============================] - 78s 19ms/step - loss: 0.1422 - accuracy: 0.9538\n",
      "Epoch 6/10\n",
      "4160/4160 [==============================] - 77s 18ms/step - loss: 0.1019 - accuracy: 0.9714\n",
      "Epoch 7/10\n",
      "4160/4160 [==============================] - 78s 19ms/step - loss: 0.0891 - accuracy: 0.9709\n",
      "Epoch 8/10\n",
      "4160/4160 [==============================] - 81s 19ms/step - loss: 0.0709 - accuracy: 0.9800\n",
      "Epoch 9/10\n",
      "4160/4160 [==============================] - 80s 19ms/step - loss: 0.0512 - accuracy: 0.9853\n",
      "Epoch 10/10\n",
      "4160/4160 [==============================] - 83s 20ms/step - loss: 0.0325 - accuracy: 0.9918\n"
     ]
    }
   ],
   "source": [
    "filepath = os.getcwd() + '/all_signals/'  \n",
    "train_data = np.load(filepath + 'traindata.npy')\n",
    "train_labels = np.load(filepath + 'trainlabels.npy')\n",
    "test_data = np.load(filepath + 'testdata.npy')\n",
    "test_labels = np.load(filepath + 'testlabels.npy')\n",
    "\n",
    "BATCH_SIZE = 32  \n",
    "EPOCHS = 10\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.getcwd(), \n",
    "        monitor='val_loss',\n",
    "        save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='acc', patience=1)\n",
    "]\n",
    "#print(model.summary())\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam', metrics=['accuracy'])\n",
    "# , class_mode=\"categorical\"\n",
    "history = model.fit(\n",
    "    train_data, train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(filepath + 'modelv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.63585829e-08, 2.00858949e-06, 3.18024043e-08, ...,\n",
       "        4.34333611e-11, 1.48890633e-08, 9.80987080e-12],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.36493705e-10, 1.53833228e-14, 1.00000000e+00],\n",
       "       [1.29319775e-11, 9.99620199e-01, 5.08623632e-09, ...,\n",
       "        9.72288638e-11, 1.48961766e-22, 3.14388355e-17],\n",
       "       ...,\n",
       "       [2.22508034e-14, 9.99795258e-01, 2.26166130e-09, ...,\n",
       "        6.43965697e-12, 1.03820589e-26, 3.12845653e-20],\n",
       "       [4.28014530e-08, 2.97376096e-06, 4.12355412e-08, ...,\n",
       "        7.63205480e-06, 5.28755598e-04, 1.25415783e-04],\n",
       "       [9.99655128e-01, 2.95392596e-07, 1.90549831e-22, ...,\n",
       "        6.90560607e-22, 2.56549399e-25, 6.58829578e-29]], dtype=float32)"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01662065472851679, 0.9970873594284058]"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(test_data, test_labels, verbose=0)\n",
    "print('Test loss: {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 Records: Without Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6240/6240 [==============================] - 109s 17ms/step - loss: 1.8722 - accuracy: 0.3700\n",
      "Epoch 2/10\n",
      "6240/6240 [==============================] - 165s 26ms/step - loss: 0.7021 - accuracy: 0.7763\n",
      "Epoch 3/10\n",
      "6240/6240 [==============================] - 141s 23ms/step - loss: 0.2862 - accuracy: 0.9154\n",
      "Epoch 4/10\n",
      "6240/6240 [==============================] - 127s 20ms/step - loss: 0.1745 - accuracy: 0.9470\n",
      "Epoch 5/10\n",
      "6240/6240 [==============================] - 152s 24ms/step - loss: 0.1186 - accuracy: 0.9649\n",
      "Epoch 6/10\n",
      "6240/6240 [==============================] - 155s 25ms/step - loss: 0.1034 - accuracy: 0.9707\n",
      "Epoch 7/10\n",
      "6240/6240 [==============================] - 142s 23ms/step - loss: 0.0606 - accuracy: 0.9853\n",
      "Epoch 8/10\n",
      "6240/6240 [==============================] - 129s 21ms/step - loss: 0.0554 - accuracy: 0.9853\n",
      "Epoch 9/10\n",
      "6240/6240 [==============================] - 116s 19ms/step - loss: 0.0576 - accuracy: 0.9840\n",
      "Epoch 10/10\n",
      "6240/6240 [==============================] - 116s 19ms/step - loss: 0.0424 - accuracy: 0.9886\n",
      "Test loss: 0.0162384338288\n",
      "Test accuracy: 0.994174778461\n"
     ]
    }
   ],
   "source": [
    "filepath = os.getcwd() + '/all_signals/'  \n",
    "train_data = np.load(filepath + 'traindata.npy')\n",
    "train_labels = np.load(filepath + 'trainlabels.npy')\n",
    "test_data = np.load(filepath + 'testdata.npy')\n",
    "test_labels = np.load(filepath + 'testlabels.npy')\n",
    "\n",
    "BATCH_SIZE = 32  \n",
    "EPOCHS = 10\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.getcwd(), \n",
    "        monitor='val_loss',\n",
    "        save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='acc', patience=1)\n",
    "]\n",
    "#print(model.summary())\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam', metrics=['accuracy'])\n",
    "# , class_mode=\"categorical\"\n",
    "history = model.fit(\n",
    "    train_data, train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list,\n",
    ")\n",
    "model.save(filepath + 'modelv1')\n",
    "score = model.evaluate(test_data, test_labels, verbose=0)\n",
    "print('Test loss: {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Records: With Butterworth Bandpass Filtering (LC=5, HC=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandPassFilter(signal):\n",
    "\n",
    "    fs=360.0\n",
    "    lowcut=5\n",
    "    highcut=35\n",
    "    nyq=0.5*fs\n",
    "    low=lowcut/nyq\n",
    "    high=highcut/nyq\n",
    "\n",
    "    order=5\n",
    "\n",
    "    b,a=scipy.signal.butter(order, [low,high],'bandpass',analog=False)\n",
    "    # b,a=scipy.signal.butter(order, low, btype='low', analog=False)\n",
    "    y=scipy.signal.filtfilt(b,a,signal,axis=0)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_signals(records): #There's 48 total records. If not specified, we'll process every single record \n",
    "    signals = []\n",
    "    for e in records:\n",
    "        if '-' in e: continue      \n",
    "        sig, fields = wfdb.rdsamp(e, channels=[0]) \n",
    "        sig=bandPassFilter(sig)\n",
    "        signals.append(np.array([x[0] for x in sig])) \n",
    "    return  signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.01010096, -0.00771209, -0.00467382, ...,  0.64464978,\n",
       "         0.37907896,  0.11872799]),\n",
       " array([-0.02953028, -0.03458173, -0.03885724, ..., -0.16741208,\n",
       "        -0.09235667, -0.00331153]),\n",
       " array([ 0.01089271,  0.01107314,  0.01163944, ..., -0.12748186,\n",
       "        -0.08008317, -0.02382439]),\n",
       " array([ 0.03478523,  0.03464661,  0.03388298, ..., -0.17284671,\n",
       "        -0.10080889, -0.01532633]),\n",
       " array([-0.06135154, -0.06063585, -0.06392351, ..., -0.01291801,\n",
       "         0.00370686,  0.02272317])]"
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signals=get_filtered_signals(records)\n",
    "signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2/all_signals/100.csv\n",
      "/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2/all_signals/101.csv\n",
      "/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2/all_signals/102.csv\n",
      "/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2/all_signals/103.csv\n",
      "/Users/aditikumari/Desktop/SJSU Courses/Fall 2019/295-A/cmpe-295/Dataset1/qrs_detection_approach_2/all_signals/104.csv\n"
     ]
    }
   ],
   "source": [
    "INDEX_DIFF = 1249\n",
    "for record,signal in zip(records, signals):\n",
    "    number = 0\n",
    "    record = record.split(\"/\",-1)[1]\n",
    "    index = 0\n",
    "    l = []\n",
    "    sub_filepath = os.path.join(filepath, record+'.csv')\n",
    "    print(sub_filepath)\n",
    "    for x in range(520):\n",
    "            segmented = None\n",
    "            label=int(record)%100\n",
    "            segmented = np.append(signal[index:index + INDEX_DIFF + 1], [label])  \n",
    "            l.append(segmented)\n",
    "            index += INDEX_DIFF + 1\n",
    "            number += 1\n",
    "    np.savetxt(sub_filepath, l, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520\n",
      "('Unique values of test', (array([0, 1, 2, 3, 4]), array([103, 103, 103, 103, 103])), 515)\n",
      "('Original Train Labels:', array([0, 2, 2, ..., 0, 0, 1]), 2080)\n",
      "('Label encoded Train Labels:', array([0, 2, 2, ..., 0, 0, 1]), 2080)\n",
      "('Train Labels after keras:', array([[1., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       ...,\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.]]))\n",
      "('Length in one list of train_label:', 5)\n",
      "('Original test Labels:', array([3, 0, 1, 3, 0, 2, 3, 0, 3, 4, 3, 4, 2, 2, 4, 2, 1, 4, 0, 3, 3, 0,\n",
      "       0, 2, 0, 4, 4, 1, 0, 1, 0, 1, 3, 2, 2, 1, 0, 3, 4, 3, 3, 2, 2, 3,\n",
      "       2, 1, 1, 1, 3, 1, 3, 4, 3, 0, 3, 3, 4, 4, 0, 4, 4, 4, 4, 0, 0, 2,\n",
      "       0, 4, 0, 4, 0, 1, 1, 1, 0, 4, 1, 2, 2, 2, 0, 1, 4, 2, 1, 0, 0, 3,\n",
      "       3, 2, 2, 4, 0, 4, 4, 2, 3, 4, 4, 1, 2, 4, 4, 1, 2, 4, 3, 2, 2, 4,\n",
      "       2, 0, 2, 3, 3, 4, 1, 4, 3, 0, 4, 4, 3, 4, 4, 0, 3, 3, 1, 2, 2, 4,\n",
      "       3, 0, 2, 3, 1, 4, 2, 0, 1, 2, 2, 1, 3, 3, 2, 1, 2, 1, 1, 1, 3, 1,\n",
      "       4, 4, 1, 0, 1, 4, 1, 2, 0, 1, 2, 1, 4, 4, 2, 4, 3, 4, 0, 2, 3, 1,\n",
      "       2, 1, 4, 0, 3, 1, 2, 0, 2, 0, 0, 0, 0, 3, 1, 3, 2, 3, 1, 4, 0, 4,\n",
      "       4, 0, 2, 2, 1, 0, 3, 2, 4, 0, 4, 4, 4, 2, 0, 2, 0, 3, 0, 4, 0, 0,\n",
      "       2, 3, 0, 3, 2, 0, 4, 0, 4, 0, 1, 4, 1, 0, 4, 1, 1, 2, 3, 4, 0, 0,\n",
      "       3, 0, 1, 3, 4, 2, 2, 4, 2, 3, 2, 3, 2, 3, 2, 3, 2, 0, 4, 1, 1, 3,\n",
      "       3, 0, 1, 3, 1, 4, 0, 2, 0, 2, 0, 3, 0, 0, 4, 3, 1, 2, 4, 4, 3, 4,\n",
      "       3, 1, 1, 0, 2, 1, 0, 3, 4, 3, 1, 0, 1, 1, 3, 2, 3, 1, 0, 1, 2, 0,\n",
      "       0, 0, 2, 2, 0, 1, 1, 0, 2, 3, 1, 2, 1, 3, 1, 4, 4, 4, 3, 2, 1, 2,\n",
      "       4, 1, 4, 0, 3, 4, 1, 3, 1, 2, 1, 4, 1, 3, 1, 1, 2, 4, 3, 1, 1, 4,\n",
      "       4, 2, 3, 1, 3, 0, 0, 2, 4, 2, 3, 0, 4, 0, 3, 2, 4, 2, 0, 2, 3, 4,\n",
      "       1, 1, 3, 4, 2, 0, 4, 0, 3, 3, 1, 3, 2, 3, 3, 2, 4, 2, 3, 2, 1, 2,\n",
      "       4, 2, 2, 1, 0, 0, 3, 1, 3, 4, 2, 0, 2, 0, 1, 3, 4, 0, 1, 3, 0, 3,\n",
      "       1, 4, 1, 1, 2, 0, 4, 4, 0, 1, 3, 1, 1, 2, 2, 1, 2, 3, 3, 0, 0, 2,\n",
      "       0, 2, 1, 4, 3, 3, 4, 2, 1, 3, 3, 3, 4, 0, 1, 0, 2, 0, 4, 0, 4, 2,\n",
      "       0, 0, 0, 3, 1, 3, 3, 2, 1, 4, 0, 0, 1, 3, 1, 1, 4, 1, 3, 1, 0, 3,\n",
      "       1, 2, 2, 0, 3, 4, 3, 0, 2, 4, 2, 1, 4, 1, 3, 4, 2, 1, 4, 3, 0, 2,\n",
      "       2, 4, 2, 4, 4, 0, 1, 3, 0]))\n",
      "('Test Lables after encoding:', array([[0., 0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0., 0.]]))\n"
     ]
    }
   ],
   "source": [
    "no_records=5\n",
    "partition_data(no_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2080/2080 [==============================] - 48s 23ms/step - loss: 0.3467 - accuracy: 0.8707\n",
      "Epoch 2/10\n",
      "2080/2080 [==============================] - 44s 21ms/step - loss: 0.1662 - accuracy: 0.9428\n",
      "Epoch 3/10\n",
      "2080/2080 [==============================] - 43s 20ms/step - loss: 0.0853 - accuracy: 0.9793\n",
      "Epoch 4/10\n",
      "2080/2080 [==============================] - 42s 20ms/step - loss: 0.0332 - accuracy: 0.9937\n",
      "Epoch 5/10\n",
      "2080/2080 [==============================] - 43s 21ms/step - loss: 0.0423 - accuracy: 0.9885\n",
      "Epoch 6/10\n",
      "2080/2080 [==============================] - 47s 23ms/step - loss: 0.0335 - accuracy: 0.9899\n",
      "Epoch 7/10\n",
      "2080/2080 [==============================] - 46s 22ms/step - loss: 0.0322 - accuracy: 0.9918\n",
      "Epoch 8/10\n",
      "2080/2080 [==============================] - 42s 20ms/step - loss: 0.0237 - accuracy: 0.9952\n",
      "Epoch 9/10\n",
      "2080/2080 [==============================] - 44s 21ms/step - loss: 0.0132 - accuracy: 0.9976\n",
      "Epoch 10/10\n",
      "2080/2080 [==============================] - 41s 19ms/step - loss: 0.0166 - accuracy: 0.9971\n",
      "Test loss: 0.00815003574718\n",
      "Test accuracy: 0.998058259487\n"
     ]
    }
   ],
   "source": [
    "filepath = os.getcwd() + '/all_signals/'  \n",
    "train_data = np.load(filepath + 'traindata.npy')\n",
    "train_labels = np.load(filepath + 'trainlabels.npy')\n",
    "test_data = np.load(filepath + 'testdata.npy')\n",
    "test_labels = np.load(filepath + 'testlabels.npy')\n",
    "\n",
    "BATCH_SIZE = 32  \n",
    "EPOCHS = 10\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.getcwd(), \n",
    "        monitor='val_loss',\n",
    "        save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='acc', patience=1)\n",
    "]\n",
    "#print(model.summary())\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam', metrics=['accuracy'])\n",
    "# , class_mode=\"categorical\"\n",
    "history = model.fit(\n",
    "    train_data, train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list,\n",
    ")\n",
    "model.save(filepath + 'modelv1')\n",
    "score = model.evaluate(test_data, test_labels, verbose=0)\n",
    "print('Test loss: {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Records: With Bandpass Filtering (LC=5, HC=35), EKF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Record 100', (65000,))\n",
      "('Record 101', (65000,))\n",
      "('Record 102', (65000,))\n",
      "('Record 103', (65000,))\n",
      "('Record 104', (65000,))\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "ab=[]\n",
    "my_data = genfromtxt('b100.csv', delimiter=',')\n",
    "ab.append(my_data)\n",
    "print(\"Record 100\", my_data.shape)\n",
    "my_data = genfromtxt('b101.csv', delimiter=',')\n",
    "ab.append(my_data)\n",
    "print(\"Record 101\", my_data.shape)\n",
    "my_data = genfromtxt('b102.csv', delimiter=',')\n",
    "ab.append(my_data)\n",
    "print(\"Record 102\", my_data.shape)\n",
    "my_data = genfromtxt('b103.csv', delimiter=',')\n",
    "ab.append(my_data)\n",
    "print(\"Record 103\", my_data.shape)\n",
    "my_data = genfromtxt('b104.csv', delimiter=',')\n",
    "print(\"Record 104\", my_data.shape)\n",
    "ab.append(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 704,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-712-474d8242b812>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-712-474d8242b812>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    print(type(l))\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "INDEX_DIFF = 1249\n",
    "for record,signal in zip(records, ab):\n",
    "    number = 0\n",
    "    record = record.split(\"/\",-1)[1]\n",
    "    index = 0\n",
    "    l = []\n",
    "    sub_filepath = os.path.join(filepath, record+'.csv')\n",
    "    print(sub_filepath)\n",
    "    for x in range(520):\n",
    "            segmented = None\n",
    "            label=int(record)%100\n",
    "            segmented = np.append(signal[index:index + INDEX_DIFF + 1], [label])  \n",
    "            l.append(segmented)\n",
    "            index += INDEX_DIFF + 1\n",
    "            number += 1\n",
    "    print(type(l))\n",
    "    np.savetxt(sub_filepath, l, delimiter=\",\",fmt='%.18e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-707-dc23cb339856>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpartition_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_records_for_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-675-2e1270573b0a>\u001b[0m in \u001b[0;36mpartition_data\u001b[0;34m(no_records_for_input)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'100.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdata_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "partition_data(no_records_for_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.getcwd() + '/all_signals/'  \n",
    "train_data = np.load(filepath + 'traindata.npy')\n",
    "train_labels = np.load(filepath + 'trainlabels.npy')\n",
    "test_data = np.load(filepath + 'testdata.npy')\n",
    "test_labels = np.load(filepath + 'testlabels.npy')\n",
    "\n",
    "BATCH_SIZE = 32  \n",
    "EPOCHS = 10\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.getcwd(), \n",
    "        monitor='val_loss',\n",
    "        save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='acc', patience=1)\n",
    "]\n",
    "#print(model.summary())\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam', metrics=['accuracy'])\n",
    "# , class_mode=\"categorical\"\n",
    "history = model.fit(\n",
    "    train_data, train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list,\n",
    ")\n",
    "model.save(filepath + 'modelv1')\n",
    "score = model.evaluate(test_data, test_labels, verbose=0)\n",
    "print('Test loss: {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 Records: With Butterworth Bandpass Filtering (LC=0.5, HC=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandPassFilter(signal):\n",
    "\n",
    "    fs=360.0\n",
    "    lowcut=0.5\n",
    "    highcut=35\n",
    "    nyq=0.5*fs\n",
    "    low=lowcut/nyq\n",
    "    high=highcut/nyq\n",
    "\n",
    "    order=5\n",
    "\n",
    "    b,a=scipy.signal.butter(order, [low,high],'bandpass',analog=False)\n",
    "    # b,a=scipy.signal.butter(order, low, btype='low', analog=False)\n",
    "    y=scipy.signal.filtfilt(b,a,signal,axis=0)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_signals(records): #There's 48 total records. If not specified, we'll process every single record \n",
    "    signals = []\n",
    "    for e in records:\n",
    "        if '-' in e: continue      \n",
    "        sig, fields = wfdb.rdsamp(e, channels=[0]) \n",
    "        sig=bandPassFilter(sig)\n",
    "        signals.append(np.array([x[0] for x in sig])) \n",
    "    return  signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.00401118, 0.00381769, 0.004142  , ..., 0.70660807, 0.38909999,\n",
       "        0.07688043]),\n",
       " array([ 0.0451091 ,  0.04327919,  0.04187249, ..., -0.23148651,\n",
       "        -0.14296673, -0.03995675]),\n",
       " array([-0.04011141, -0.04112327, -0.04171711, ..., -0.07609153,\n",
       "        -0.0247146 ,  0.035547  ]),\n",
       " array([-0.03564507, -0.03573953, -0.03620041, ..., -0.07558115,\n",
       "        -0.00568501,  0.07785934]),\n",
       " array([0.08514511, 0.09891162, 0.10987026, ..., 0.00959723, 0.02737455,\n",
       "        0.04790717]),\n",
       " array([ 0.02106767,  0.02022005,  0.0189846 , ..., -0.74820853,\n",
       "        -0.48936295, -0.21245723]),\n",
       " array([0.02996645, 0.02811818, 0.02677998, ..., 0.75725085, 0.42117103,\n",
       "        0.07156621]),\n",
       " array([-0.23798556, -0.2442712 , -0.25279807, ..., -0.6184167 ,\n",
       "        -0.41829007, -0.18622993]),\n",
       " array([-0.00560313, -0.00365397, -0.00248333, ..., -0.08397672,\n",
       "        -0.06295569, -0.03966707]),\n",
       " array([ 0.11177771,  0.11180669,  0.11167761, ...,  0.07648736,\n",
       "        -0.02847683, -0.1508537 ]),\n",
       " array([-0.00685918, -0.0074    , -0.00743089, ...,  0.05506292,\n",
       "         0.01400231, -0.04040807]),\n",
       " array([0.00402273, 0.00648401, 0.00818914, ..., 0.31879144, 0.17919988,\n",
       "        0.01439273]),\n",
       " array([-0.09035008, -0.09169119, -0.09299852, ...,  1.20653624,\n",
       "         0.62930476, -0.01892773]),\n",
       " array([ 0.05211013,  0.05223943,  0.05238972, ..., -0.29552921,\n",
       "        -0.181206  , -0.04887139]),\n",
       " array([-0.05374824, -0.05324222, -0.05317784, ...,  0.39173724,\n",
       "         0.23573408,  0.05370783])]"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signals=get_filtered_signals(records)\n",
    "signals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520\n",
      "('Unique values of test', (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 11, 12, 13, 14, 15]), array([103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103,\n",
      "       103, 103])), 1545)\n",
      "('Original Train Labels:', array([ 2,  2, 14, ...,  1,  3,  3]), 6240)\n",
      "('Label encoded Train Labels:', array([ 2,  2, 13, ...,  1,  3,  3]), 6240)\n",
      "('Train Labels after keras:', array([[0., 0., 1., ..., 0., 0., 0.],\n",
      "       [0., 0., 1., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 1., 0.],\n",
      "       ...,\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]))\n",
      "('Length in one list of train_label:', 15)\n",
      "('Original test Labels:', array([ 1, 11,  5, ..., 13,  8,  7]))\n",
      "('Test Lables after encoding:', array([[0., 1., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]))\n"
     ]
    }
   ],
   "source": [
    "partition_data(no_records_for_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6240/6240 [==============================] - 104s 17ms/step - loss: 0.0542 - accuracy: 0.9872\n",
      "Epoch 2/10\n",
      "6240/6240 [==============================] - 105s 17ms/step - loss: 0.0577 - accuracy: 0.9846\n",
      "Epoch 3/10\n",
      "6240/6240 [==============================] - 111s 18ms/step - loss: 0.0312 - accuracy: 0.9920\n",
      "Epoch 4/10\n",
      "6240/6240 [==============================] - 107s 17ms/step - loss: 0.0258 - accuracy: 0.9926\n",
      "Epoch 5/10\n",
      "6240/6240 [==============================] - 138s 22ms/step - loss: 0.0267 - accuracy: 0.9920\n",
      "Epoch 6/10\n",
      "6240/6240 [==============================] - 111s 18ms/step - loss: 0.0281 - accuracy: 0.9917\n",
      "Epoch 7/10\n",
      "6240/6240 [==============================] - 112s 18ms/step - loss: 0.0209 - accuracy: 0.9939\n",
      "Epoch 8/10\n",
      "6240/6240 [==============================] - 120s 19ms/step - loss: 0.0306 - accuracy: 0.9921\n",
      "Epoch 9/10\n",
      "6240/6240 [==============================] - 107s 17ms/step - loss: 0.0229 - accuracy: 0.9939\n",
      "Epoch 10/10\n",
      "6240/6240 [==============================] - 116s 19ms/step - loss: 0.0187 - accuracy: 0.9957\n",
      "Test loss: 0.0636125090741\n",
      "Test accuracy: 0.98511326313\n"
     ]
    }
   ],
   "source": [
    "filepath = os.getcwd() + '/all_signals/'  \n",
    "train_data = np.load(filepath + 'traindata.npy')\n",
    "train_labels = np.load(filepath + 'trainlabels.npy')\n",
    "test_data = np.load(filepath + 'testdata.npy')\n",
    "test_labels = np.load(filepath + 'testlabels.npy')\n",
    "\n",
    "BATCH_SIZE = 32  \n",
    "EPOCHS = 10\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.getcwd(), \n",
    "        monitor='val_loss',\n",
    "        save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='acc', patience=1)\n",
    "]\n",
    "#print(model.summary())\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam', metrics=['accuracy'])\n",
    "# , class_mode=\"categorical\"\n",
    "history = model.fit(\n",
    "    train_data, train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list,\n",
    ")\n",
    "model.save(filepath + 'modelv1')\n",
    "score = model.evaluate(test_data, test_labels, verbose=0)\n",
    "print('Test loss: {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 Records: With Butterworth Bandpass Filtering (LC=5, HC=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandPassFilter1(signal):\n",
    "\n",
    "    fs=360.0\n",
    "    lowcut=5\n",
    "    highcut=35\n",
    "    nyq=0.5*fs\n",
    "    low=lowcut/nyq\n",
    "    high=highcut/nyq\n",
    "\n",
    "    order=5\n",
    "\n",
    "    b,a=scipy.signal.butter(order, [low,high],'bandpass',analog=False)\n",
    "    # b,a=scipy.signal.butter(order, low, btype='low', analog=False)\n",
    "    y=scipy.signal.filtfilt(b,a,signal,axis=0)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_signals1(records): #There's 48 total records. If not specified, we'll process every single record \n",
    "    signals = []\n",
    "    for e in records:\n",
    "        if '-' in e: continue      \n",
    "        sig, fields = wfdb.rdsamp(e, channels=[0]) \n",
    "        sig=bandPassFilter1(sig)\n",
    "        signals.append(np.array([x[0] for x in sig])) \n",
    "    return  signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.01010096, -0.00771209, -0.00467382, ...,  0.64464978,\n",
       "         0.37907896,  0.11872799]),\n",
       " array([-0.02953028, -0.03458173, -0.03885724, ..., -0.16741208,\n",
       "        -0.09235667, -0.00331153]),\n",
       " array([ 0.01089271,  0.01107314,  0.01163944, ..., -0.12748186,\n",
       "        -0.08008317, -0.02382439]),\n",
       " array([ 0.03478523,  0.03464661,  0.03388298, ..., -0.17284671,\n",
       "        -0.10080889, -0.01532633]),\n",
       " array([-0.06135154, -0.06063585, -0.06392351, ..., -0.01291801,\n",
       "         0.00370686,  0.02272317]),\n",
       " array([ 0.00068197, -0.00815308, -0.01739228, ..., -0.38966551,\n",
       "        -0.20183369,  0.00299211]),\n",
       " array([0.04791321, 0.04386458, 0.04009262, ..., 0.55917936, 0.3136078 ,\n",
       "        0.05628048]),\n",
       " array([ 0.01955314, -0.01703321, -0.05491302, ..., -0.36116655,\n",
       "        -0.19188536,  0.00794695]),\n",
       " array([ 0.01869765,  0.02907332,  0.03831102, ..., -0.02253497,\n",
       "        -0.00971262,  0.00557182]),\n",
       " array([-0.01853909, -0.02146129, -0.02444456, ...,  0.16756318,\n",
       "         0.05695276, -0.06894291]),\n",
       " array([0.00480276, 0.00666952, 0.00907246, ..., 0.11939397, 0.06969369,\n",
       "        0.00682041]),\n",
       " array([0.01362386, 0.01848146, 0.02233503, ..., 0.29637331, 0.16962932,\n",
       "        0.01831411]),\n",
       " array([-0.00319514, -0.00517967, -0.00713217, ...,  1.31523883,\n",
       "         0.74201491,  0.09442028]),\n",
       " array([-0.00743522, -0.00897703, -0.01041186, ..., -0.22203018,\n",
       "        -0.12427597, -0.00910564]),\n",
       " array([-0.00987023, -0.00858893, -0.00779783, ...,  0.37744725,\n",
       "         0.24212088,  0.08102836])]"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signals=get_filtered_signals1(records)\n",
    "signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520\n",
      "('Unique values of test', (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 11, 12, 13, 14, 15]), array([103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103,\n",
      "       103, 103])), 1545)\n",
      "('Original Train Labels:', array([13,  7,  1, ...,  0, 15,  8]), 6240)\n",
      "('Label encoded Train Labels:', array([12,  7,  1, ...,  0, 14,  8]), 6240)\n",
      "('Train Labels after keras:', array([[0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 1.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]))\n",
      "('Length in one list of train_label:', 15)\n",
      "('Original test Labels:', array([11,  5, 13, ..., 15, 13, 15]))\n",
      "('Test Lables after encoding:', array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 1.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 1.]]))\n"
     ]
    }
   ],
   "source": [
    "partition_data(no_records_for_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6240/6240 [==============================] - 194s 31ms/step - loss: 0.0185 - accuracy: 0.9952\n",
      "Epoch 2/10\n",
      "6240/6240 [==============================] - 147s 24ms/step - loss: 0.0239 - accuracy: 0.9937\n",
      "Epoch 3/10\n",
      "6240/6240 [==============================] - 132s 21ms/step - loss: 0.0176 - accuracy: 0.9957\n",
      "Epoch 4/10\n",
      "6240/6240 [==============================] - 125s 20ms/step - loss: 0.0155 - accuracy: 0.9947\n",
      "Epoch 5/10\n",
      "6240/6240 [==============================] - 128s 21ms/step - loss: 0.0175 - accuracy: 0.9950\n",
      "Epoch 6/10\n",
      "6240/6240 [==============================] - 133s 21ms/step - loss: 0.0139 - accuracy: 0.9952\n",
      "Epoch 7/10\n",
      "6240/6240 [==============================] - 164s 26ms/step - loss: 0.0105 - accuracy: 0.9971\n",
      "Epoch 8/10\n",
      "6240/6240 [==============================] - 144s 23ms/step - loss: 0.0311 - accuracy: 0.9910\n",
      "Epoch 9/10\n",
      "6240/6240 [==============================] - 145s 23ms/step - loss: 0.0184 - accuracy: 0.9947\n",
      "Epoch 10/10\n",
      "6240/6240 [==============================] - 130s 21ms/step - loss: 0.0111 - accuracy: 0.9968\n",
      "Test loss: 0.014687959561\n",
      "Test accuracy: 0.998705506325\n"
     ]
    }
   ],
   "source": [
    "filepath = os.getcwd() + '/all_signals/'  \n",
    "train_data = np.load(filepath + 'traindata.npy')\n",
    "train_labels = np.load(filepath + 'trainlabels.npy')\n",
    "test_data = np.load(filepath + 'testdata.npy')\n",
    "test_labels = np.load(filepath + 'testlabels.npy')\n",
    "\n",
    "BATCH_SIZE = 32  \n",
    "EPOCHS = 10\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.getcwd(), \n",
    "        monitor='val_loss',\n",
    "        save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='acc', patience=1)\n",
    "]\n",
    "#print(model.summary())\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam', metrics=['accuracy'])\n",
    "# , class_mode=\"categorical\"\n",
    "history = model.fit(\n",
    "    train_data, train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list,\n",
    ")\n",
    "model.save(filepath + 'modelv1')\n",
    "score = model.evaluate(test_data, test_labels, verbose=0)\n",
    "print('Test loss: {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
